name: Rust vs Python ML Benchmark CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly benchmarks
    - cron: '0 2 * * 0'

env:
  CARGO_TERM_COLOR: always
  PYTHON_VERSION: '3.11'

jobs:
  test:
    name: Test Suite
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache Rust dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cargo/registry
          ~/.cargo/git
          target
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.toml') }}
        restore-keys: |
          ${{ runner.os }}-cargo-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8 black mypy
    
    - name: Install Rust dependencies
      run: |
        cargo install cargo-audit
        cargo install cargo-tarpaulin
    
    - name: Lint Python code
      run: |
        flake8 src/python/ scripts/ tests/
        black --check src/python/ scripts/ tests/
        mypy src/python/ scripts/ --ignore-missing-imports
    
    - name: Lint Rust code
      run: |
        cargo clippy --all-targets --all-features -- -D warnings
        cargo fmt -- --check
    
    - name: Run Python tests
      run: |
        pytest tests/ -v --cov=src/python --cov-report=xml --cov-report=html
    
    - name: Run Rust tests
      run: |
        cargo test --all
    
    - name: Security audit
      run: |
        cargo audit
        safety check
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  build:
    name: Build Benchmarks
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Build Python benchmarks
      run: |
        python -m py_compile src/python/classical_ml/*.py
        python -m py_compile src/python/deep_learning/*.py
        python -m py_compile src/python/reinforcement_learning/*.py
        python -m py_compile src/python/llm/*.py
    
    - name: Build Rust benchmarks
      run: |
        find src/rust -name "Cargo.toml" -execdir cargo build --release \;
    
    - name: Build Docker images
      run: |
        docker build -t rust-ml-benchmark:python containers/python/
        docker build -t rust-ml-benchmark:rust containers/rust/

  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    needs: build
    
    strategy:
      matrix:
        benchmark: [regression, svm, cnn, dqn, transformer]
        language: [python, rust]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Set up Rust
      uses: actions-rs/toolchain@v1
      with:
        toolchain: stable
        override: true
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run Python benchmarks
      if: matrix.language == 'python'
      run: |
        case ${{ matrix.benchmark }} in
          regression)
            python src/python/classical_ml/regression_benchmark.py \
              --dataset boston_housing --algorithm linear \
              --mode training --run-id test-run
            ;;
          svm)
            python src/python/classical_ml/svm_benchmark.py \
              --dataset iris --algorithm svc \
              --mode training --run-id test-run
            ;;
          cnn)
            python src/python/deep_learning/cnn_benchmark.py \
              --dataset mnist --architecture lenet \
              --mode training --run-id test-run
            ;;
          dqn)
            python src/python/reinforcement_learning/dqn_benchmark.py \
              --environment CartPole-v1 --mode training --run-id test-run
            ;;
          transformer)
            python src/python/llm/transformer_benchmark.py \
              --model distilbert-base-uncased --task text-generation \
              --mode inference --run-id test-run
            ;;
        esac
    
    - name: Run Rust benchmarks
      if: matrix.language == 'rust'
      run: |
        case ${{ matrix.benchmark }} in
          regression)
            cd src/rust/classical_ml/regression_benchmark/
            cargo run --release -- \
              --dataset boston_housing --algorithm linear \
              --mode training --run-id test-run
            ;;
          svm)
            cd src/rust/classical_ml/svm_benchmark/
            cargo run --release -- \
              --dataset iris --algorithm svc \
              --mode training --run-id test-run
            ;;
          cnn)
            cd src/rust/deep_learning/cnn_benchmark/
            cargo run --release -- \
              --dataset mnist --architecture lenet \
              --mode training --run-id test-run
            ;;
        esac
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.language }}-${{ matrix.benchmark }}
        path: |
          *_results.json
          results/

  analysis:
    name: Statistical Analysis
    runs-on: ubuntu-latest
    needs: benchmark
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        path: benchmark-results/
    
    - name: Run statistical analysis
      run: |
        python scripts/perform_statistical_analysis.py \
          --results benchmark-results/ \
          --output analysis_results.json
    
    - name: Generate visualizations
      run: |
        python scripts/create_visualizations.py \
          --statistical-results analysis_results.json \
          --output-visualizations visualization_metadata.json
    
    - name: Upload analysis results
      uses: actions/upload-artifact@v3
      with:
        name: analysis-results
        path: |
          analysis_results.json
          visualization_metadata.json
          plots/

  report:
    name: Generate Report
    runs-on: ubuntu-latest
    needs: [analysis, benchmark]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Download analysis results
      uses: actions/download-artifact@v3
      with:
        name: analysis-results
        path: analysis-results/
    
    - name: Assess ecosystem maturity
      run: |
        python scripts/assess_ecosystem_maturity.py \
          --output ecosystem_assessment.json \
          --include-recommendations
    
    - name: Generate final report
      run: |
        python scripts/generate_final_report.py \
          --statistical-results analysis-results/analysis_results.json \
          --ecosystem-assessment ecosystem_assessment.json \
          --framework-evaluation framework_evaluation.json \
          --recommendations recommendations.json \
          --output final_report.md
    
    - name: Upload final report
      uses: actions/upload-artifact@v3
      with:
        name: final-report
        path: |
          final_report.md
          ecosystem_assessment.json

  deploy:
    name: Deploy Results
    runs-on: ubuntu-latest
    needs: report
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Download final report
      uses: actions/download-artifact@v3
      with:
        name: final-report
        path: reports/
    
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./reports
        publish_branch: gh-pages
    
    - name: Create Release
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: v${{ github.run_number }}
        release_name: Benchmark Results v${{ github.run_number }}
        body: |
          Automated benchmark results for commit ${{ github.sha }}
          
          - Statistical analysis completed
          - Ecosystem assessment performed
          - Comprehensive report generated
        draft: false
        prerelease: false

  notify:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: [deploy]
    if: always()
    
    steps:
    - name: Notify on Success
      if: success()
      run: |
        echo "Benchmark pipeline completed successfully!"
        echo "Results available at: https://${{ github.repository }}/actions/runs/${{ github.run_id }}"
    
    - name: Notify on Failure
      if: failure()
      run: |
        echo "Benchmark pipeline failed!"
        echo "Check logs at: https://${{ github.repository }}/actions/runs/${{ github.run_id }}" 