benchmarks:
  classical_ml:
    regression:
      datasets: 
        - "boston_housing"
        - "california_housing" 
        - "synthetic_linear"
        - "synthetic_nonlinear"
        - "synthetic_sparse"
        - "diabetes"
        - "wine_quality"
      algorithms: 
        - "linear"
        - "ridge" 
        - "lasso"
        - "elastic_net"
      metrics: 
        - "rmse"
        - "mae"
        - "r2_score"
        - "mape"
        - "explained_variance"
        - "residual_std"
        - "residual_skew"
        - "residual_kurtosis"
        - "mse"
        - "training_time"
        - "model_sparsity"
        - "n_nonzero_coefficients"
      hyperparameters:
        linear: {}
        ridge: 
          alpha: [0.1, 1.0, 10.0, 100.0]
        lasso: 
          alpha: [0.1, 1.0, 10.0, 100.0]
        elastic_net: 
          alpha: [0.1, 1.0, 10.0]
          l1_ratio: [0.1, 0.5, 0.9]
      repetitions: 15
      cross_validation_folds: 5
      statistical_significance_level: 0.05
      power_analysis:
        effect_size_threshold: 0.2
        power_threshold: 0.8
        minimum_sample_size: 30

    svm:
      datasets: 
        - "iris"
        - "wine" 
        - "breast_cancer"
        - "digits"
        - "synthetic_classification"
        - "adult"
        - "covertype"
      algorithms: 
        - "svc"
        - "linearsvc"
        - "nusvc"
        - "svr"
      metrics: 
        - "accuracy"
        - "f1_score"
        - "precision"
        - "recall"
        - "auc_roc"
        - "auc_pr"
        - "training_time"
        - "inference_latency"
      hyperparameters:
        svc: 
          C: [0.1, 1.0, 10.0, 100.0]
          kernel: ["rbf", "linear", "poly"]
          gamma: ["scale", "auto", 0.001, 0.01, 0.1]
        linearsvc: 
          C: [0.1, 1.0, 10.0, 100.0]
          loss: ["hinge", "squared_hinge"]
        nusvc: 
          nu: [0.1, 0.5, 0.9]
          kernel: ["rbf", "linear"]
        svr:
          C: [0.1, 1.0, 10.0]
          epsilon: [0.01, 0.1, 0.5]
      repetitions: 15
      cross_validation_folds: 5

    clustering:
      datasets: 
        - "iris"
        - "wine"
        - "digits"
        - "synthetic_clusters"
        - "blobs_2d"
        - "blobs_3d"
        - "moons"
        - "circles"
      algorithms: 
        - "kmeans"
        - "dbscan"
        - "agglomerative"
        - "spectral"
        - "gaussian_mixture"
      metrics: 
        - "silhouette_score"
        - "adjusted_rand_score"
        - "normalized_mutual_info"
        - "calinski_harabasz_score"
        - "davies_bouldin_score"
        - "inertia"
        - "n_clusters"
        - "training_time"
      hyperparameters:
        kmeans: 
          n_clusters: [2, 3, 4, 5, 8, 10]
          init: ["k-means++", "random"]
          n_init: [10, 20]
        dbscan: 
          eps: [0.1, 0.5, 1.0, 2.0]
          min_samples: [2, 5, 10, 20]
        agglomerative: 
          n_clusters: [2, 3, 4, 5, 8]
          linkage: ["ward", "complete", "average"]
        spectral:
          n_clusters: [2, 3, 4, 5, 8]
          affinity: ["rbf", "nearest_neighbors"]
        gaussian_mixture:
          n_components: [2, 3, 4, 5, 8]
          covariance_type: ["full", "tied", "diag", "spherical"]
      repetitions: 15
      cross_validation_folds: 5

  deep_learning:
    cnn:
      datasets: 
        - "mnist"
        - "cifar10"
        - "cifar100"
        - "fashion_mnist"
        - "imagenet_subset"
      architectures: 
        - "lenet"
        - "simple_cnn"
        - "resnet18"
        - "resnet34"
        - "vgg16"
        - "alexnet"
        - "mobilenet"
      metrics: 
        - "accuracy"
        - "loss"
        - "training_time"
        - "inference_latency"
        - "throughput"
        - "memory_usage"
        - "gpu_utilization"
        - "convergence_epochs"
        - "learning_curve"
      hyperparameters:
        epochs: [5, 10, 20, 50]
        batch_size: [16, 32, 64, 128, 256]
        learning_rate: [0.001, 0.01, 0.1]
        optimizer: ["adam", "sgd", "rmsprop"]
        weight_decay: [0.0, 0.0001, 0.001]
        dropout_rate: [0.0, 0.2, 0.5]
      repetitions: 10
      early_stopping_patience: 10
      learning_rate_scheduling: true
      data_augmentation: true

    rnn:
      datasets: 
        - "sine_wave"
        - "sequence_classification"
        - "time_series"
        - "text_classification"
        - "sentiment_analysis"
        - "language_modeling"
      architectures: 
        - "lstm"
        - "gru"
        - "rnn"
        - "bidirectional_lstm"
        - "stacked_lstm"
        - "attention_lstm"
      metrics: 
        - "accuracy"
        - "loss"
        - "perplexity"
        - "bleu_score"
        - "training_time"
        - "inference_latency"
        - "memory_usage"
        - "convergence_steps"
      hyperparameters:
        epochs: [5, 10, 20, 50]
        batch_size: [16, 32, 64, 128]
        learning_rate: [0.001, 0.01, 0.1]
        hidden_size: [64, 128, 256, 512]
        num_layers: [1, 2, 3, 4]
        dropout_rate: [0.0, 0.2, 0.5]
        sequence_length: [50, 100, 200, 500]
      repetitions: 10
      gradient_clipping: true
      teacher_forcing: true

  reinforcement_learning:
    dqn:
      environments: 
        - "CartPole-v1"
        - "LunarLander-v2"
        - "Acrobot-v1"
        - "MountainCar-v0"
        - "Pendulum-v1"
        - "BipedalWalker-v3"
      algorithms: 
        - "dqn"
        - "ddqn"
        - "dueling_dqn"
        - "prioritized_dqn"
        - "rainbow_dqn"
      metrics: 
        - "mean_reward"
        - "success_rate"
        - "episode_length"
        - "training_time"
        - "inference_latency"
        - "memory_usage"
        - "convergence_episodes"
        - "exploration_rate"
        - "loss"
        - "q_value_estimate"
      hyperparameters:
        total_timesteps: [10000, 50000, 100000, 500000]
        learning_rate: [0.0001, 0.001, 0.01]
        buffer_size: [10000, 50000, 100000, 1000000]
        batch_size: [32, 64, 128, 256]
        gamma: [0.9, 0.95, 0.99]
        epsilon_start: [1.0, 0.9]
        epsilon_end: [0.01, 0.05]
        epsilon_decay: [0.995, 0.99, 0.985]
        target_update_freq: [100, 500, 1000]
        learning_starts: [1000, 5000, 10000]
      repetitions: 10
      evaluation_episodes: 100
      render_environment: false

    policy_gradient:
      environments: 
        - "CartPole-v1"
        - "LunarLander-v2"
        - "Acrobot-v1"
        - "MountainCar-v0"
        - "Pendulum-v1"
        - "BipedalWalker-v3"
      algorithms: 
        - "a2c"
        - "ppo"
        - "trpo"
        - "sac"
        - "td3"
      metrics: 
        - "mean_reward"
        - "success_rate"
        - "episode_length"
        - "training_time"
        - "inference_latency"
        - "memory_usage"
        - "convergence_episodes"
        - "policy_loss"
        - "value_loss"
        - "entropy"
      hyperparameters:
        total_episodes: [100, 500, 1000, 5000]
        learning_rate: [0.0001, 0.001, 0.01]
        batch_size: [32, 64, 128, 256]
        gamma: [0.9, 0.95, 0.99]
        gae_lambda: [0.9, 0.95, 0.99]
        clip_ratio: [0.1, 0.2, 0.3]
        value_coef: [0.5, 1.0, 2.0]
        entropy_coef: [0.01, 0.05, 0.1]
        max_grad_norm: [0.5, 1.0, 2.0]
      repetitions: 10
      evaluation_episodes: 100

  llm:
    gpt2:
      models: 
        - "gpt2"
        - "gpt2-medium"
        - "gpt2-large"
        - "gpt2-xl"
      tasks: 
        - "text_generation"
        - "language_modeling"
        - "perplexity"
        - "next_token_prediction"
      metrics: 
        - "tokens_per_second"
        - "latency_p50"
        - "latency_p95"
        - "latency_p99"
        - "memory_usage"
        - "gpu_memory_usage"
        - "throughput"
        - "perplexity"
        - "accuracy"
        - "bleu_score"
      batch_sizes: [1, 4, 8, 16, 32]
      sequence_lengths: [128, 256, 512, 1024, 2048]
      hyperparameters:
        temperature: [0.7, 1.0, 1.2]
        top_k: [40, 50, 100]
        top_p: [0.9, 0.95, 0.99]
        repetition_penalty: [1.0, 1.1, 1.2]
        max_length: [50, 100, 200]
        do_sample: [true, false]
      repetitions: 15
      warmup_runs: 10
      benchmark_runs: 100

    bert:
      models: 
        - "bert-base-uncased"
        - "bert-large-uncased"
        - "distilbert-base-uncased"
        - "roberta-base"
        - "albert-base-v2"
      tasks: 
        - "question_answering"
        - "sentiment_classification"
        - "named_entity_recognition"
        - "text_classification"
        - "token_classification"
      metrics: 
        - "tokens_per_second"
        - "latency_p50"
        - "latency_p95"
        - "latency_p99"
        - "memory_usage"
        - "gpu_memory_usage"
        - "throughput"
        - "accuracy"
        - "f1_score"
        - "exact_match"
      batch_sizes: [1, 4, 8, 16, 32]
      sequence_lengths: [128, 256, 512, 1024]
      hyperparameters:
        learning_rate: [1e-5, 2e-5, 5e-5]
        weight_decay: [0.0, 0.01, 0.1]
        warmup_steps: [0, 100, 500]
        gradient_accumulation_steps: [1, 2, 4]
        max_grad_norm: [1.0, 2.0, 5.0]
      repetitions: 15
      warmup_runs: 10
      benchmark_runs: 100

# Statistical Analysis Configuration
statistical_analysis:
  confidence_level: 0.95
  alpha: 0.05
  multiple_comparison_correction: "bonferroni"  # Options: bonferroni, holm, fdr
  effect_size_measures:
    - "cohens_d"
    - "cliffs_delta"
    - "hedges_g"
  normality_tests:
    - "shapiro_wilk"
    - "anderson_darling"
    - "kolmogorov_smirnov"
  power_analysis:
    effect_size_threshold: 0.2
    power_threshold: 0.8
    minimum_sample_size: 30
  outlier_detection:
    method: "iqr"  # Options: iqr, zscore, isolation_forest
    threshold: 1.5
  bootstrap:
    n_bootstrap_samples: 10000
    confidence_interval_method: "percentile"  # Options: percentile, bca, t

# Resource Monitoring Configuration
resource_monitoring:
  sampling_interval_ms: 100
  enable_gpu_monitoring: true
  enable_energy_monitoring: false
  enable_network_monitoring: false
  memory_tracking:
    peak_memory: true
    average_memory: true
    memory_timeline: true
  cpu_tracking:
    utilization: true
    frequency: true
    temperature: false
  gpu_tracking:
    utilization: true
    memory_usage: true
    temperature: false
    power_consumption: false

# Reproducibility Configuration
reproducibility:
  deterministic_seeds:
    numpy: 42
    torch: 42
    tensorflow: 42
    random: 42
  version_pinning: true
  environment_isolation: true
  data_checksums: true
  hardware_configuration_tracking: true
  complete_metadata_capture: true

# Quality Assurance Configuration
quality_assurance:
  code_quality:
    linting: true
    type_checking: true
    security_auditing: true
    code_coverage_threshold: 80
  testing:
    unit_tests: true
    integration_tests: true
    performance_tests: true
    regression_tests: true
  validation:
    result_validation: true
    outlier_detection: true
    sanity_checks: true
    cross_validation: true
  documentation:
    inline_documentation: true
    api_documentation: true
    user_guides: true
    technical_specifications: true 