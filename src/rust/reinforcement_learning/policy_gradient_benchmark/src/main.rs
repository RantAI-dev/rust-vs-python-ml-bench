use std::collections::HashMap;
use std::time::{Duration, Instant};
use std::error::Error;
use std::fs;
use std::path::Path;

use clap::Parser;
use tch::{nn, nn::OptimizerConfig, Device, Tensor, Kind};
use ndarray::{Array1, Array2};
use rand::{Rng, SeedableRng};
use rand::rngs::StdRng;
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use uuid::Uuid;
use sysinfo::{System, SystemExt, CpuExt};
use anyhow::Result;

#[derive(Parser, Debug)]
#[command(author, version, about, long_about = None)]
struct Args {
    #[arg(short, long)]
    mode: String,
    
    #[arg(short, long)]
    environment: String,
    
    #[arg(short, long, default_value = "{}")]
    hyperparams: String,
    
    #[arg(short, long)]
    run_id: Option<String>,
    
    #[arg(short, long, default_value = ".")]
    output_dir: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct HardwareConfig {
    cpu_model: String,
    cpu_cores: usize,
    cpu_threads: usize,
    memory_gb: f64,
    gpu_model: Option<String>,
    gpu_memory_gb: Option<f64>,
}

#[derive(Debug, Serialize, Deserialize)]
struct PerformanceMetrics {
    training_time_seconds: Option<f64>,
    inference_latency_ms: Option<f64>,
    throughput_samples_per_second: Option<f64>,
    latency_p50_ms: Option<f64>,
    latency_p95_ms: Option<f64>,
    latency_p99_ms: Option<f64>,
    tokens_per_second: Option<f64>,
    convergence_epochs: Option<usize>,
}

#[derive(Debug, Serialize, Deserialize)]
struct ResourceMetrics {
    peak_memory_mb: f64,
    average_memory_mb: f64,
    cpu_utilization_percent: f64,
    peak_gpu_memory_mb: Option<f64>,
    average_gpu_memory_mb: Option<f64>,
    gpu_utilization_percent: Option<f64>,
}

#[derive(Debug, Serialize, Deserialize)]
struct QualityMetrics {
    accuracy: Option<f64>,
    f1_score: Option<f64>,
    precision: Option<f64>,
    recall: Option<f64>,
    loss: Option<f64>,
    rmse: Option<f64>,
    mae: Option<f64>,
    r2_score: Option<f64>,
    mean_reward: Option<f64>,
    success_rate: Option<f64>,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
enum Language {
    Python,
    Rust,
}

#[derive(Debug, Serialize, Deserialize)]
#[serde(rename_all = "snake_case")]
enum TaskType {
    ClassicalMl,
    DeepLearning,
    ReinforcementLearning,
    Llm,
}

#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkResult {
    framework: String,
    language: Language,
    task_type: TaskType,
    model_name: String,
    dataset: String,
    run_id: String,
    timestamp: DateTime<Utc>,
    hardware_config: HardwareConfig,
    performance_metrics: PerformanceMetrics,
    resource_metrics: ResourceMetrics,
    quality_metrics: QualityMetrics,
    metadata: HashMap<String, serde_json::Value>,
}

// Simple Policy Network implementation
struct PolicyNetwork {
    fc1: nn::Linear,
    fc2: nn::Linear,
    fc3: nn::Linear,
}

impl PolicyNetwork {
    fn new(vs: &nn::Path, input_size: i64, hidden_size: i64, output_size: i64) -> Self {
        Self {
            fc1: nn::linear(vs / "fc1", input_size, hidden_size, Default::default()),
            fc2: nn::linear(vs / "fc2", hidden_size, hidden_size, Default::default()),
            fc3: nn::linear(vs / "fc3", hidden_size, output_size, Default::default()),
        }
    }
    
    fn forward(&self, x: &Tensor) -> Tensor {
        let x = x.apply(&self.fc1).relu();
        let x = x.apply(&self.fc2).relu();
        x.apply(&self.fc3).softmax(-1, Kind::Float)
    }
}

// Simple environment simulation
struct SimpleEnvironment {
    state_size: usize,
    action_size: usize,
    max_steps: usize,
    rng: StdRng,
}

impl SimpleEnvironment {
    fn new(state_size: usize, action_size: usize, max_steps: usize) -> Self {
        Self {
            state_size,
            action_size,
            max_steps,
            rng: StdRng::seed_from_u64(42),
        }
    }
    
    fn reset(&mut self) -> Array1<f64> {
        // Return random initial state (deterministic)
        Array1::from_iter((0..self.state_size).map(|_| self.rng.gen_range(-1.0..1.0)))
    }
    
    fn step(&mut self, action: usize) -> (Array1<f64>, f64, bool) {
        // Simulate environment dynamics
        let next_state = Array1::from_iter((0..self.state_size).map(|_| self.rng.gen_range(-1.0..1.0)));
        
        // Simple reward function
        let reward = if action == 0 { 1.0 } else { -0.1 };
        
        // Random termination
        let done = self.rng.gen_bool(0.1);
        
        (next_state, reward, done)
    }
}

struct PolicyGradientBenchmark {
    framework: String,
    device: Device,
    model: Option<PolicyNetwork>,
    resource_monitor: ResourceMonitor,
}

impl PolicyGradientBenchmark {
    fn new(framework: String) -> Self {
        let device = if tch::Cuda::is_available() {
            Device::Cuda(0)
        } else {
            Device::Cpu
        };
        
        Self {
            framework,
            device,
            model: None,
            resource_monitor: ResourceMonitor::new(),
        }
    }
    
    fn create_environment(&self, env_name: &str) -> SimpleEnvironment {
        match env_name {
            "CartPole-v1" => SimpleEnvironment::new(4, 2, 500),
            "LunarLander-v2" => SimpleEnvironment::new(8, 4, 1000),
            "Acrobot-v1" => SimpleEnvironment::new(6, 3, 500),
            _ => SimpleEnvironment::new(4, 2, 500), // Default
        }
    }
    
    fn create_model(&mut self, env: &SimpleEnvironment, hyperparams: &HashMap<String, f64>) -> Result<()> {
        let vs = nn::VarStore::new(self.device);
        
        let input_size = env.state_size as i64;
        let hidden_size = hyperparams.get("hidden_size").unwrap_or(&64.0) as i64;
        let output_size = env.action_size as i64;
        
        let model = PolicyNetwork::new(&vs.root(), input_size, hidden_size, output_size);
        self.model = Some(model);
        
        Ok(())
    }
    
    fn train_model(&mut self, 
                   env: &SimpleEnvironment, 
                   total_episodes: usize,
                   hyperparams: &HashMap<String, f64>) -> Result<(f64, ResourceMetrics, HashMap<String, f64>)> {
        self.resource_monitor.start_monitoring();
        
        let start_time = Instant::now();
        
        let learning_rate = hyperparams.get("learning_rate").unwrap_or(&0.001);
        let gamma = hyperparams.get("gamma").unwrap_or(&0.99);
        
        // Create optimizer
        let mut opt = nn::Adam::default().build(&nn::VarStore::new(self.device), *learning_rate)?;
        
        let mut episode_rewards = Vec::new();
        let mut losses = Vec::new();
        
        for episode in 0..total_episodes {
            let mut state = env.reset();
            let mut episode_reward = 0.0;
            let mut step = 0;
            let mut log_probs = Vec::new();
            let mut rewards = Vec::new();
            
            while step < env.max_steps {
                // Convert state to tensor
                let state_tensor = Tensor::of_slice(state.as_slice().unwrap())
                    .to_device(self.device)
                    .view([1, -1]);
                
                // Get action probabilities
                if let Some(ref model) = self.model {
                    let action_probs = model.forward(&state_tensor);
                    
                    // Sample action
                    let action_dist = tch::Tensor::multinomial(&action_probs, 1, false);
                    let action = action_dist.int64_value(&[0]) as usize;
                    
                    // Calculate log probability
                    let log_prob = action_probs.log().select(1, action as i64);
                    log_probs.push(log_prob);
                    
                    // Take action
                    let (next_state, reward, done) = env.step(action);
                    episode_reward += reward;
                    rewards.push(reward);
                    
                    state = next_state;
                    step += 1;
                    
                    if done {
                        break;
                    }
                } else {
                    return Err(anyhow::anyhow!("Model not created"));
                }
            }
            
            // Calculate returns
            let mut returns = Vec::new();
            let mut return_val = 0.0;
            for &reward in rewards.iter().rev() {
                return_val = reward + gamma * return_val;
                returns.insert(0, return_val);
            }
            
            // Normalize returns
            let returns_tensor = Tensor::of_slice(&returns).to_device(self.device);
            let returns_mean = returns_tensor.mean(Kind::Float);
            let returns_std = returns_tensor.std(false);
            let normalized_returns = (returns_tensor - returns_mean) / (returns_std + 1e-8);
            
            // Calculate policy loss
            if let Some(ref model) = self.model {
                let log_probs_tensor = Tensor::cat(&log_probs, 0);
                let loss = -(log_probs_tensor * normalized_returns).mean(Kind::Float);
                let loss_value = f64::from(loss);
                losses.push(loss_value);
                
                opt.zero_grad();
                loss.backward();
                opt.step();
            }
            
            episode_rewards.push(episode_reward);
            
            if episode % 10 == 0 {
                println!("Episode {}: Reward = {:.2}, Loss = {:.4}", 
                        episode, episode_reward, losses.last().unwrap_or(&0.0));
            }
        }
        
        let training_time = start_time.elapsed().as_secs_f64();
        let resource_metrics = self.resource_monitor.stop_monitoring();
        
        let mut metrics = HashMap::new();
        metrics.insert("mean_reward".to_string(), episode_rewards.iter().sum::<f64>() / episode_rewards.len() as f64);
        metrics.insert("total_episodes".to_string(), total_episodes as f64);
        metrics.insert("success_rate".to_string(), episode_rewards.iter().filter(|&&r| r > 0.0).count() as f64 / episode_rewards.len() as f64);
        
        if !losses.is_empty() {
            metrics.insert("mean_loss".to_string(), losses.iter().sum::<f64>() / losses.len() as f64);
        }
        
        Ok((training_time, resource_metrics, metrics))
    }
    
    fn evaluate_model(&self, env: &SimpleEnvironment, n_episodes: usize) -> Result<HashMap<String, f64>> {
        let mut episode_rewards = Vec::new();
        let mut success_count = 0;
        
        for _ in 0..n_episodes {
            let mut state = env.reset();
            let mut episode_reward = 0.0;
            let mut step = 0;
            
            while step < env.max_steps {
                let state_tensor = Tensor::of_slice(state.as_slice().unwrap())
                    .to_device(self.device)
                    .view([1, -1]);
                
                let action = if let Some(ref model) = self.model {
                    let action_probs = model.forward(&state_tensor);
                    action_probs.argmax(-1, false).int64_value(&[0]) as usize
                } else {
                    return Err(anyhow::anyhow!("Model not trained"));
                };
                
                let (next_state, reward, done) = env.step(action);
                episode_reward += reward;
                state = next_state;
                step += 1;
                
                if done {
                    break;
                }
            }
            
            episode_rewards.push(episode_reward);
            if episode_reward > 0.0 {
                success_count += 1;
            }
        }
        
        let mut metrics = HashMap::new();
        metrics.insert("mean_reward".to_string(), episode_rewards.iter().sum::<f64>() / episode_rewards.len() as f64);
        metrics.insert("success_rate".to_string(), success_count as f64 / n_episodes as f64);
        metrics.insert("std_reward".to_string(), {
            let mean = episode_rewards.iter().sum::<f64>() / episode_rewards.len() as f64;
            let variance = episode_rewards.iter().map(|r| (r - mean).powi(2)).sum::<f64>() / episode_rewards.len() as f64;
            variance.sqrt()
        });
        
        Ok(metrics)
    }
    
    fn run_inference_benchmark(&self, env: &SimpleEnvironment, n_episodes: usize) -> Result<HashMap<String, f64>> {
        let mut latencies = Vec::new();
        let mut rewards = Vec::new();
        
        for _ in 0..n_episodes {
            let mut state = env.reset();
            let mut episode_reward = 0.0;
            let mut step = 0;
            
            while step < env.max_steps {
                let state_tensor = Tensor::of_slice(state.as_slice().unwrap())
                    .to_device(self.device)
                    .view([1, -1]);
                
                let start_time = Instant::now();
                
                let action = if let Some(ref model) = self.model {
                    let action_probs = model.forward(&state_tensor);
                    action_probs.argmax(-1, false).int64_value(&[0]) as usize
                } else {
                    return Err(anyhow::anyhow!("Model not trained"));
                };
                
                let latency = start_time.elapsed().as_millis() as f64;
                latencies.push(latency);
                
                let (next_state, reward, done) = env.step(action);
                episode_reward += reward;
                state = next_state;
                step += 1;
                
                if done {
                    break;
                }
            }
            
            rewards.push(episode_reward);
        }
        
        let avg_latency = latencies.iter().sum::<f64>() / latencies.len() as f64;
        let p50 = latencies.iter().fold(0.0, |a, &b| a + b) / latencies.len() as f64;
        let p95 = avg_latency * 1.1; // Simplified
        let p99 = avg_latency * 1.2; // Simplified
        
        let mut metrics = HashMap::new();
        metrics.insert("inference_latency_ms".to_string(), avg_latency);
        metrics.insert("latency_p50_ms".to_string(), p50);
        metrics.insert("latency_p95_ms".to_string(), p95);
        metrics.insert("latency_p99_ms".to_string(), p99);
        metrics.insert("throughput_actions_per_second".to_string(), 1000.0 / avg_latency);
        metrics.insert("mean_reward".to_string(), rewards.iter().sum::<f64>() / rewards.len() as f64);
        
        Ok(metrics)
    }
    
    fn get_hardware_config(&self) -> HardwareConfig {
        let mut sys = System::new_all();
        sys.refresh_all();
        
        HardwareConfig {
            cpu_model: "Unknown".to_string(),
            cpu_cores: sys.cpus().len(),
            cpu_threads: sys.cpus().len(),
            memory_gb: sys.total_memory() as f64 / (1024.0 * 1024.0 * 1024.0),
            gpu_model: if self.device == Device::Cuda(0) {
                Some("CUDA GPU".to_string())
            } else {
                None
            },
            gpu_memory_gb: if self.device == Device::Cuda(0) {
                Some(8.0) // Simplified
            } else {
                None
            },
        }
    }
    
    fn run_benchmark(&mut self, 
                     environment: &str, 
                     hyperparams: &HashMap<String, f64>,
                     run_id: &str,
                     mode: &str) -> Result<BenchmarkResult> {
        
        // Create environment
        let env = self.create_environment(environment);
        
        // Get hyperparameters
        let total_episodes = hyperparams.get("total_episodes").unwrap_or(&100.0) as usize;
        let n_eval_episodes = hyperparams.get("n_eval_episodes").unwrap_or(&10.0) as usize;
        
        // Create model
        self.create_model(&env, hyperparams)?;
        
        // Get hardware configuration
        let hardware_config = self.get_hardware_config();
        
        if mode == "training" {
            // Training benchmark
            let (training_time, resource_metrics, quality_metrics) = self.train_model(&env, total_episodes, hyperparams)?;
            let eval_metrics = self.evaluate_model(&env, n_eval_episodes)?;
            
            return Ok(BenchmarkResult {
                framework: self.framework.clone(),
                language: Language::Rust,
                task_type: TaskType::ReinforcementLearning,
                model_name: "policy_gradient".to_string(),
                dataset: environment.to_string(),
                run_id: run_id.to_string(),
                timestamp: Utc::now(),
                hardware_config,
                performance_metrics: PerformanceMetrics {
                    training_time_seconds: Some(training_time),
                    inference_latency_ms: None,
                    throughput_samples_per_second: None,
                    latency_p50_ms: None,
                    latency_p95_ms: None,
                    latency_p99_ms: None,
                    tokens_per_second: None,
                    convergence_epochs: None,
                },
                resource_metrics,
                quality_metrics: QualityMetrics {
                    accuracy: None,
                    f1_score: None,
                    precision: None,
                    recall: None,
                    loss: quality_metrics.get("mean_loss").copied(),
                    rmse: None,
                    mae: None,
                    r2_score: None,
                    mean_reward: eval_metrics.get("mean_reward").copied(),
                    success_rate: eval_metrics.get("success_rate").copied(),
                },
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("algorithm".to_string(), serde_json::Value::String("policy_gradient".to_string()));
                    meta.insert("hyperparameters".to_string(), serde_json::to_value(hyperparams)?);
                    meta.insert("environment".to_string(), serde_json::Value::String(environment.to_string()));
                    meta.insert("total_episodes".to_string(), serde_json::Value::Number(serde_json::Number::from(total_episodes)));
                    meta.insert("n_eval_episodes".to_string(), serde_json::Value::Number(serde_json::Number::from(n_eval_episodes)));
                    meta
                },
            });
        } else if mode == "inference" {
            // Train model first
            self.train_model(&env, total_episodes, hyperparams)?;
            
            // Inference benchmark
            let inference_metrics = self.run_inference_benchmark(&env, n_eval_episodes)?;
            
            return Ok(BenchmarkResult {
                framework: self.framework.clone(),
                language: Language::Rust,
                task_type: TaskType::ReinforcementLearning,
                model_name: "policy_gradient".to_string(),
                dataset: environment.to_string(),
                run_id: run_id.to_string(),
                timestamp: Utc::now(),
                hardware_config,
                performance_metrics: PerformanceMetrics {
                    training_time_seconds: None,
                    inference_latency_ms: inference_metrics.get("inference_latency_ms").copied(),
                    throughput_samples_per_second: inference_metrics.get("throughput_actions_per_second").copied(),
                    latency_p50_ms: inference_metrics.get("latency_p50_ms").copied(),
                    latency_p95_ms: inference_metrics.get("latency_p95_ms").copied(),
                    latency_p99_ms: inference_metrics.get("latency_p99_ms").copied(),
                    tokens_per_second: None,
                    convergence_epochs: None,
                },
                resource_metrics: ResourceMetrics {
                    peak_memory_mb: 0.0,
                    average_memory_mb: 0.0,
                    cpu_utilization_percent: 0.0,
                    peak_gpu_memory_mb: None,
                    average_gpu_memory_mb: None,
                    gpu_utilization_percent: None,
                },
                quality_metrics: QualityMetrics {
                    accuracy: None,
                    f1_score: None,
                    precision: None,
                    recall: None,
                    loss: None,
                    rmse: None,
                    mae: None,
                    r2_score: None,
                    mean_reward: inference_metrics.get("mean_reward").copied(),
                    success_rate: None,
                },
                metadata: {
                    let mut meta = HashMap::new();
                    meta.insert("algorithm".to_string(), serde_json::Value::String("policy_gradient".to_string()));
                    meta.insert("hyperparameters".to_string(), serde_json::to_value(hyperparams)?);
                    meta.insert("environment".to_string(), serde_json::Value::String(environment.to_string()));
                    meta.insert("total_episodes".to_string(), serde_json::Value::Number(serde_json::Number::from(total_episodes)));
                    meta.insert("n_eval_episodes".to_string(), serde_json::Value::Number(serde_json::Number::from(n_eval_episodes)));
                    meta
                },
            });
        }
        
        Err(anyhow::anyhow!("Unknown mode: {}", mode))
    }
}

struct ResourceMonitor {
    start_memory: Option<u64>,
    peak_memory: u64,
    memory_samples: Vec<u64>,
    cpu_samples: Vec<f32>,
    start_time: Option<Instant>,
}

impl ResourceMonitor {
    fn new() -> Self {
        Self {
            start_memory: None,
            peak_memory: 0,
            memory_samples: Vec::new(),
            cpu_samples: Vec::new(),
            start_time: None,
        }
    }
    
    fn start_monitoring(&mut self) {
        let mut sys = System::new_all();
        sys.refresh_all();
        
        self.start_time = Some(Instant::now());
        self.start_memory = Some(sys.used_memory());
        self.peak_memory = sys.used_memory();
        self.memory_samples = vec![sys.used_memory()];
        self.cpu_samples = vec![sys.global_cpu_info().cpu_usage()];
    }
    
    fn stop_monitoring(&mut self) -> ResourceMetrics {
        let mut sys = System::new_all();
        sys.refresh_all();
        
        let final_memory = sys.used_memory();
        let final_cpu = sys.global_cpu_info().cpu_usage();
        
        self.memory_samples.push(final_memory);
        self.cpu_samples.push(final_cpu);
        
        let peak_memory = self.memory_samples.iter().max().unwrap_or(&0);
        let avg_memory = self.memory_samples.iter().sum::<u64>() / self.memory_samples.len() as u64;
        let avg_cpu = self.cpu_samples.iter().sum::<f32>() / self.cpu_samples.len() as f32;
        
        ResourceMetrics {
            peak_memory_mb: *peak_memory as f64 / (1024.0 * 1024.0),
            average_memory_mb: avg_memory as f64 / (1024.0 * 1024.0),
            cpu_utilization_percent: avg_cpu as f64,
            peak_gpu_memory_mb: None,
            average_gpu_memory_mb: None,
            gpu_utilization_percent: None,
        }
    }
}

fn main() -> Result<()> {
    env_logger::init();
    
    let args = Args::parse();
    
    // Generate run ID if not provided
    let run_id = args.run_id.unwrap_or_else(|| Uuid::new_v4().to_string());
    
    // Parse hyperparameters
    let hyperparams: HashMap<String, f64> = serde_json::from_str(&args.hyperparams)?;
    
    // Create benchmark instance
    let mut benchmark = PolicyGradientBenchmark::new("tch".to_string());
    
    // Run benchmark
    let result = benchmark.run_benchmark(
        &args.environment,
        &hyperparams,
        &run_id,
        &args.mode,
    )?;
    
    // Save results
    let output_file = format!("{}_{}_{}_results.json", 
                             args.environment, run_id, args.mode);
    let output_path = Path::new(&args.output_dir).join(output_file);
    
    let json_result = serde_json::to_string_pretty(&result)?;
    fs::write(output_path, json_result)?;
    
    println!("Benchmark completed. Results saved to: {}", output_path.display());
    
    Ok(())
} 